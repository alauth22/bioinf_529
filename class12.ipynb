{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before Class\n",
    "In class today we will be implementing a Markov chain to process sentences\n",
    "Prior to class, please do the following:\n",
    "1. Review slides on Markov chains in detail\n",
    "* Explore using the python dict() structure and how a dict() can contain nested dict() structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Conceptually understand Markov Chains\n",
    "* Implement a Markov Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lectures that Markov Chains represent a series of events following the Markov Property: future states are memory-less in that they depend only on the current state. This can be expanded to the idea of variable order Markov models where there is a variable-length memory (eg. 1st order Markov Model). Markov models consist of fully observable states. A common example of this is in predicting the weather: We can clearly see the current weather and would like to predict tomorrow's weather. As shown in the slides, this is also applicable to biology with one case being CpG islands. \n",
    "\n",
    "Our goal today will be to implement a Markov model built from words. For our example text, we will use the classic example of Dr. Seuss because of the repetitive nature of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Markov model\n",
    "\n",
    "For our initial implementation of the Markov Model, we will use the simple example of Dr. Seuss: \"One fish two fish red fish blue fish.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_markov_model(markov_model, new_text):\n",
    "    '''\n",
    "    Function to build or add to a 1st order Markov model given a string of text\n",
    "    We will store the markov model as a dictionary of dictionaries\n",
    "    The key in the outer dictionary represents the current state\n",
    "    and the inner dictionary represents the next state with their contents containing\n",
    "    the transition probabilities.\n",
    "    Note: This would be easier to read if we were to build a class representation\n",
    "           of the model rather than a dictionary of dictionaries, but for simplicitiy\n",
    "           our implementation will just use this structure.\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "        new_text (str): a string to build or add to the markov_model\n",
    "\n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated markov_model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Add artificial states for start and end\n",
    "        For each word in text:\n",
    "            Increment markov_model[word][next_word]\n",
    "        \n",
    "    '''\n",
    "    #goal to build just first-order Markov Model, that's it! Pretty restrictive and boring....\n",
    "    \n",
    "    #first order in case of DNA: previous base is considered while second order with DNA: two previous bases are considered. \n",
    "    \n",
    "    #turn the new_text string into a list\n",
    "    new_text_lst = new_text.split()\n",
    "    # print(new_text_lst)\n",
    "\n",
    "    #add artificial begin and end states\n",
    "    new_text_lst.insert(0, \"START\")\n",
    "    new_text_lst.append(\"END\")\n",
    "    # print(new_text_lst)\n",
    "    \n",
    "    #now have to iterate through each word in the text. \n",
    "    #do indexes to keep track. \n",
    "    \n",
    "    #needs to be -1 because you'll be out of range since you have been adding one. How are you going to account for the very last one. \n",
    "    for index in range(len(new_text_lst) - 1):\n",
    "        # print(index)\n",
    "        # print(new_text_lst[index])\n",
    "        # print(new_text_lst[index + 1])\n",
    "        #will this word be in the markov_model \n",
    "        \n",
    "        #dictionaries in python don't like to be addressed when there's no value in the dictionary itself.\n",
    "        #as we're building the dictionary model, we must keep checking as we continue to build. \n",
    "        \n",
    "        #again we're only focused on one word here, \n",
    "        #so does it have \"START\", \"one\", \"fish\", \"two\", etc. \n",
    "        if new_text_lst[index] in markov_model: #if the current word is already in the markov_model\n",
    "            #then we want to see if the next word is in the markov_model within the second dictionary. \n",
    "            if new_text_lst[index + 1] in markov_model[new_text_lst[index]]: #so if \n",
    "                #we want to increment in that frequency position. \n",
    "                markov_model[new_text_lst[index]][new_text_lst[index + 1]] += 1\n",
    "            if new_text_lst[index + 1] not in markov_model[new_text_lst[index]]:\n",
    "                #if the next word is not already in the markov_model, then we're going to add it to the markov_model by adding a frequency of 1\n",
    "                markov_model[new_text_lst[index]][new_text_lst[index + 1]] = 1\n",
    "        #now we need to check if the actual word is already in the markov model or not. If it's not, then we need to add it to the empty dictionary. \n",
    "        if new_text_lst[index] not in markov_model:\n",
    "            markov_model[new_text_lst[index]] = {}\n",
    "            markov_model[new_text_lst[index]][new_text_lst[index + 1]] = 1\n",
    "          \n",
    "    #return the dictionary markov_model \n",
    "    return markov_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, 'END': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}\n"
     ]
    }
   ],
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model(markov_model, text) #markov_model is set to equal the output of the function. \n",
    "print (markov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'START': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 2, 'Amelia': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 2}, 'Amelia': {'END': 1}}\n"
     ]
    }
   ],
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish blue fish Amelia\"\n",
    "markov_model = build_markov_model(markov_model, text) #markov_model is set to equal the output of the function. \n",
    "print (markov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"one\"][\"fish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['START',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'driving',\n",
       " 'from',\n",
       " 'Michigan',\n",
       " 'back',\n",
       " 'to',\n",
       " 'Wisconsin',\n",
       " 'END']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"I will be driving from Michigan back to Wisconsin\"\n",
    "string_list = string.split()\n",
    "string_list\n",
    "string_list.insert(0, \"START\")\n",
    "string_list\n",
    "string_list.append(\"END\")\n",
    "string_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Nth order Markov chain\n",
    "In the above model, each event or word is output from only the previous state with no memory of any prior states. While this is useful in some cases, typical biological applications of Markov chains require higher-order models to accurately capture what we know about a system. For instance, in attempting to identify coding regions of a genome, we know that open reading frames (ORFs) contain codon triplets, and so a third or sixth order Markov chain would better describe these regions. Here you will implement a generalized form of our previous Markov Chain to allow for Nth order chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_markov_model2(markov_model, text, order=1):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "    '''\n",
    "    #splitting the new_text\n",
    "    new_text_lst = text.split()\n",
    "    # print(type(new_text_lst))\n",
    "    new_text_lst.append(\"END\")\n",
    "    \n",
    "    # print(new_text_lst)  \n",
    "    \n",
    "    #order is just an integer, need to have \n",
    "    for j in range(order):\n",
    "        new_text_lst.insert(0, \"START\")\n",
    "    # print(new_text_lst)\n",
    "    #this will be the size of the order for the START for markov chain \n",
    "        \n",
    "    \n",
    "    #last time we subtracted one since it was a first order. Now we're subtracting whatever the order is: \n",
    "    for index in range(len(new_text_lst) - order): \n",
    "        #we cannot search for just one word since this is NOT a first order Markov Model, it is nth order (can be any number)\n",
    "        #it is already a list so you can just use the tuple() method. \n",
    "        \n",
    "        #i had a list at first, but realized you need a tuple to accomplish this task. \n",
    "        #tuples are hashable, but lists are not. \n",
    "        top_key_words = tuple(new_text_lst[index:index + order])\n",
    "        #we need to see if that number of order is present in the markov_model, then we can go ahead and implement the same thing. \n",
    "        # print(top_key_words)\n",
    "        \n",
    "        #need to do the same if conditions for the empty dict of dictionaries. \n",
    "        #remember that you are working with nth order, so it can be how ever many beginning states. \n",
    "        if top_key_words in markov_model:\n",
    "            if new_text_lst[index + order] in markov_model[top_key_words]:\n",
    "                #need the += increment in case there are more than one certain word that are within the inner dictionaries. \n",
    "                markov_model[top_key_words][new_text_lst[index + order]] += 1\n",
    "                \n",
    "            if new_text_lst[index + order] not in markov_model[top_key_words]:\n",
    "                markov_model[top_key_words][new_text_lst[index + order]] = 1\n",
    "                \n",
    "        if top_key_words not in markov_model:\n",
    "            markov_model[top_key_words] = {}\n",
    "            markov_model[top_key_words][new_text_lst[index + order]] = 1\n",
    "            \n",
    "        \n",
    "    return markov_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('START', 'START'): {'one': 1},\n",
       " ('START', 'one'): {'fish': 1},\n",
       " ('one', 'fish'): {'two': 1},\n",
       " ('fish', 'two'): {'fish': 1},\n",
       " ('two', 'fish'): {'red': 1},\n",
       " ('fish', 'red'): {'fish': 1},\n",
       " ('red', 'fish'): {'blue': 1},\n",
       " ('fish', 'blue'): {'fish': 1},\n",
       " ('blue', 'fish'): {'END': 1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this case the text has an order of 2 since it's looking at how many starts there are.\n",
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model2(markov_model, text, order=2)\n",
    "markov_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "{('START', 'START'): {'one': 1},\n",
    " ('START', 'one'): {'fish': 1},\n",
    " ('one', 'fish'): {'two': 1},\n",
    " ('fish', 'two'): {'fish': 1},\n",
    " ('two', 'fish'): {'red': 1},\n",
    " ('fish', 'red'): {'fish': 1},\n",
    " ('red', 'fish'): {'blue': 1},\n",
    " ('fish', 'blue'): {'fish': 1},\n",
    " ('blue', 'fish'): {'*E*': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('START', 'START'): {'one': 1},\n",
       " ('START', 'one'): {'fish': 1},\n",
       " ('one', 'fish'): {'two': 1},\n",
       " ('fish', 'two'): {'fish': 1},\n",
       " ('two', 'fish'): {'red': 1},\n",
       " ('fish', 'red'): {'fish': 1},\n",
       " ('red', 'fish'): {'blue': 1},\n",
       " ('fish', 'blue'): {'fish': 1},\n",
       " ('blue', 'fish'): {'END': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = (\"START\", \"START\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word = markov_model[tup].keys()\n",
    "next_word_lst = list(next_word)\n",
    "next_word_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('START', 'START')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f0/f31tss896y3fkq0mqk3ry0j40000gn/T/ipykernel_1421/3220530133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkov_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnext_word_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnext_word_freq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('START', 'START')"
     ]
    }
   ],
   "source": [
    "freq = markov_model[tup].values()\n",
    "next_word_freq = list(freq)\n",
    "next_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"a\", \"b\", \"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = tuple(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 2\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi', 'hi')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = []\n",
    "for index in range(num):\n",
    "    empty.append(\"hi\")\n",
    "tuple(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "full = [\"a\", \"b\", \"c\"]\n",
    "for index in range(num):\n",
    "    full.insert(0, \"hi\")\n",
    "print(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text from Markov Model\n",
    "\n",
    "Markov models are \"generative models\". That is, the probability states in the model can be used to generate output following the conditional probabilities in the model.\n",
    "\n",
    "We will now generate a sequence of text from the Markov model. For this section, I recommend using np.random.choice, which allows for you to provide a probability distribution for drawing the next edge in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word2(current_word, markov_model, seed=42):\n",
    "    '''\n",
    "    Function to randomly move a valid next state given a markov model\n",
    "    and a current state (word)\n",
    "    \n",
    "    Args: \n",
    "        current_word (tuple): a word that exists in our model\n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        next_word (str): a randomly selected next word based on transition probabilies\n",
    "        \n",
    "    Pseudocode:\n",
    "        Calculate transition probabilities for all next states from a given state (counts/sum)\n",
    "        Randomly draw from these to generate the next state\n",
    "        \n",
    "    '''\n",
    "    next_words = markov_model[current_word].keys()\n",
    "    next_words_lst = list(next_words)\n",
    "    # next_state_lst = list(next_state)\n",
    "    # print(next_state_lst)\n",
    "    \n",
    "    #now get frequencies \n",
    "    freq = markov_model[current_word].values()\n",
    "    next_state_freq = list(freq) #all the integers. \n",
    "    \n",
    "\n",
    "    # now calculate probabilities \n",
    "    for x in next_state_freq:\n",
    "        next_word_prob = [x / sum(next_state_freq)]\n",
    "        next_word_probabilities = list(next_word_prob)\n",
    "        print(next_word_probabilities)\n",
    "    \n",
    "    #their code. \n",
    "    next_words_probabilities = [x / sum(next_state_freq) for x in next_state_freq]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    #random.choice(a, size=None, replace=True, p=None)\n",
    "    #a =  random sample is generated from its elements\n",
    "    #size = Default is None, in which case a single value is returned.\n",
    "    #p = probabilities \n",
    "    \n",
    "    next_word = np.random.choice(next_words, None, p = next_words_probabilities)\n",
    "    \n",
    "    return next_word[0] #output is the inner dictionary, so you have this {\"S\": 1} and you only want the \"S\". so [0]\n",
    "#we only want to return the word, not the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_text2(markov_model, seed=42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "        \n",
    "    '''\n",
    "\n",
    "# initialize sentence at start state.\n",
    "    # need to account for an nth Markov Chain \n",
    "    test = markov_model.keys()[0] #list \n",
    "    order = len(test) #this will give me an integer \n",
    "\n",
    "    #we first need to estimate the order of the model \n",
    "    #need to convert the tuple (the keys in the first dict) into a list\n",
    "    # order_test = (list(markov_model.keys())[0])\n",
    "    # order = len(order_test)\n",
    "    \n",
    "    #we must start at the initial state. So we need to append the same order of \"START\" to a tuple \n",
    "    sentence_lst = []\n",
    "    for i in range(order):\n",
    "        sentence_lst.append(\"START\") #will make the number of \"START\"s for the order of the model \n",
    "        \n",
    "    start_tup = tuple(sentence_lst) #convert list of \"START\" from a list back to a tuple, tuples are hashable, lists are not. \n",
    "    #(\"START\", \"START\") \n",
    "\n",
    "    sentence_test = [] #empty sentence this is what we'll be returning \n",
    "    current_word = \"\"\n",
    "    \n",
    "    #we don't want the \"END\", so do a while loop, until the model generates the \"END\" state. \n",
    "    while current_word != \"END\":\n",
    "        #we're going to get the next word. \n",
    "        get_next_word(start_tup, markov_model, seed = seed)\n",
    "        \n",
    "        if current_word != \"END\":\n",
    "            sentence_test.append(current_word)\n",
    "        \n",
    "        if current_word == \"END\":\n",
    "            break \n",
    "            \n",
    "        #removing the first word from the tuple, adding our new word to the tuple, and turn it back into a tuple. \n",
    "        current = list(start_tup)\n",
    "        current.pop(0) #removes the first element. index[:1]\n",
    "        current.append(current_word) #append the next word. \n",
    "        start_tup = tuple(current) \n",
    "        \n",
    "    #need to take our original state and our next word and turn it into our next word. \n",
    "    #(\"START\", \"START\") = one\n",
    "    #(\"START\", \"one) = fish \n",
    "    \n",
    "    sentence = \"\".join(sentence_test)\n",
    "    \n",
    "    return(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_next_word(current_word, markov_model, seed=42):\n",
    "    '''\n",
    "    Function to randomly move a valid next state given a markov model\n",
    "    and a current state (word)\n",
    "    \n",
    "    Args: \n",
    "        current_word (tuple): a word that exists in our model\n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        next_word (str): a randomly selected next word based on transition probabilies\n",
    "        \n",
    "    Pseudocode:\n",
    "        Calculate transition probabilities for all next states from a given state (counts/sum)\n",
    "        Randomly draw from these to generate the next state\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #this will be the a in the np.random.choice. \n",
    "    #I want all the next_states which are within the inner dictionary. \n",
    "    #this given state will be in the current_word tuple. \n",
    "    \n",
    "    \n",
    "     # Get all of our possible next states\n",
    "    next_words = list(markov_model[current_word].keys())\n",
    "    \n",
    "    # Calculate the probabilities to move to those based on word counts\n",
    "    next_words_frequencies = list(markov_model[current_word].values())\n",
    "    next_words_probabilities = [x / sum(next_words_frequencies) for x in next_words_frequencies]\n",
    "\n",
    "    # Randomly move to the next state\n",
    "    np.random.seed(seed)\n",
    "    next_state = np.random.choice(next_words, 1, p=next_words_probabilities)\n",
    "\n",
    "    # Return next word, it returns a list. so we need to convert it from a list to a string. \n",
    "    return next_state[0]\n",
    "\n",
    "    \n",
    "def generate_random_text(markov_model, seed=42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "     # We must start at the initial state of the model\n",
    "    # estimate order\n",
    "    order = len(list(markov_model.keys())[0])\n",
    "    \n",
    "    # We must start at the initial state of the model\n",
    "    current_keyList = []\n",
    "    for i in range (order):\n",
    "        current_keyList.append(\"START\")\n",
    "        \n",
    "    current_tuple = tuple(current_keyList)\n",
    "\n",
    "    # Keeping track of the sentence as a list (ignoring the start state)\n",
    "    sentence = list()\n",
    "    current_word = ''\n",
    "\n",
    "    # Until the model generates an end state, keep adding random words\n",
    "    while current_word != \"END\":\n",
    "        current_word = get_next_word(current_tuple, markov_model, seed=seed)\n",
    "        \n",
    "        # Don't append the end state to our output\n",
    "        if current_word != \"END\":\n",
    "            sentence.append(current_word)\n",
    "            \n",
    "        current_list = list(current_tuple)\n",
    "        current_list.pop(0)\n",
    "        current_list.append(current_word)\n",
    "        current_tuple = tuple(current_list)\n",
    "\n",
    "    # Return the words with spaces between them\n",
    "    return ' '.join(sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now just add some more training data to the markov model\n",
    "markov_model = dict()\n",
    "markov_model = build_markov_model2(markov_model, \"black fish blue fish old fish new fish\")\n",
    "markov_model = build_markov_model2(markov_model, \"this one has a little car\")\n",
    "markov_model = build_markov_model2(markov_model, \"this one has a little star\")\n",
    "markov_model = build_markov_model2(markov_model, \"say what a lot of fish there are\")\n",
    "markov_model = build_markov_model2(markov_model, \"yes some are red and some are blue\")\n",
    "markov_model = build_markov_model2(markov_model, \"some are old and some are new\")\n",
    "\n",
    "print (generate_random_text(markov_model,seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a more complex text that we can use to generate more complex output\n",
    "sonet_markov_model = dict()\n",
    "file = open(\"data/sonnets.txt\", \"r\")\n",
    "sonet = \"\"\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line == \"\":\n",
    "        # Empty line so build model\n",
    "        sonet_markov_model = build_markov_model(sonet_markov_model, sonet)\n",
    "        sonet = \"\"\n",
    "    else:\n",
    "        sonet = sonet + ' ' + line\n",
    "\n",
    "print ((generate_random_text(sonet_markov_model,seed=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Four', 'Three', 'Two', 'One']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup = ((\"Four\", \"Three\", \"Two\", \"One\"))\n",
    "tup\n",
    "tup_list = list(tup)\n",
    "tup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*S*': {'one': 1},\n",
       " 'one': {'fish': 1},\n",
       " 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1},\n",
       " 'two': {'fish': 1},\n",
       " 'red': {'fish': 1},\n",
       " 'blue': {'fish': 1}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', 'red', 'blue', '*E*']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test[\"fish\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test[\"fish\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
