{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Coverage and Variant Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Before Class\n",
    "1. Install bowtie2, samtools, and bamnostic: `conda install bowtie2 samtools bamnostic`\n",
    "* Review bowtie2, samtools, and bamnostic documentation\n",
    "* Review Counter from collections\n",
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Use BWT algorithm to map reads to a genome\n",
    "* Implement 'pileup' method\n",
    "* Call variants from DNA sequencing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before class:\n",
    "`conda install bowtie2 bamnostic samtools`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/alauth/opt/anaconda3/envs/b529\n",
      "\n",
      "  added / updated specs:\n",
      "    - samtools=1.9\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    bzip2-1.0.8                |       h0d85af4_4         155 KB  conda-forge\n",
      "    cffi-1.14.4                |   py37hbddb872_0         218 KB  conda-forge\n",
      "    dbus-1.13.6                |       h2f22bb5_0         558 KB  conda-forge\n",
      "    gettext-0.19.8.1           |    haf92f58_1004         3.2 MB  conda-forge\n",
      "    glib-2.66.1                |       h39b9ebd_0         3.2 MB  conda-forge\n",
      "    htslib-1.9                 |       h3a161e8_7         1.2 MB  bioconda\n",
      "    libdeflate-1.0             |       h1de35cc_1          45 KB  bioconda\n",
      "    libedit-3.1.20191231       |       hed1e85f_2         103 KB  conda-forge\n",
      "    libffi-3.2.1               |                1          41 KB  bioconda\n",
      "    libpq-12.9                 |       h1c9f633_1         2.0 MB\n",
      "    libtiff-4.2.0              |       h1167814_3         592 KB  conda-forge\n",
      "    ncurses-6.1                |    h0a44026_1002         1.3 MB  conda-forge\n",
      "    pillow-8.2.0               |   py37hd4e48bc_1         649 KB  conda-forge\n",
      "    python-3.7.8               |hc9dea61_1_cpython        23.8 MB  conda-forge\n",
      "    qt-5.12.9                  |       h126340a_2        87.8 MB  conda-forge\n",
      "    readline-8.0               |       hed1e85f_1         255 KB  conda-forge\n",
      "    samtools-1.9               |      h8aa4d43_12         304 KB  bioconda\n",
      "    sqlite-3.37.2              |       h707629a_0         1.2 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       126.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  bzip2              conda-forge/osx-64::bzip2-1.0.8-h0d85af4_4\n",
      "  glib               conda-forge/osx-64::glib-2.66.1-h39b9ebd_0\n",
      "  htslib             bioconda/osx-64::htslib-1.9-h3a161e8_7\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  libglib-2.70.2-hf1fb8c0_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  samtools                                          1.4.1-0 --> 1.9-h8aa4d43_12\n",
      "  sqlite              conda-forge::sqlite-3.37.0-h23a322b_0 --> pkgs/main::sqlite-3.37.2-h707629a_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  libdeflate         conda-forge::libdeflate-1.8-h0d85af4_0 --> bioconda::libdeflate-1.0-h1de35cc_1\n",
      "  libffi               conda-forge::libffi-3.4.2-h0d85af4_5 --> bioconda::libffi-3.2.1-1\n",
      "  libpq                  conda-forge::libpq-13.5-hea3049e_1 --> pkgs/main::libpq-12.9-h1c9f633_1\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  cffi                                1.15.0-py37h446072c_0 --> 1.14.4-py37hbddb872_0\n",
      "  dbus                                    1.13.6-h811a1a6_3 --> 1.13.6-h2f22bb5_0\n",
      "  gettext                            0.19.8.1-hd1a6beb_1008 --> 0.19.8.1-haf92f58_1004\n",
      "  libedit                           3.1.20191231-h0678c8f_2 --> 3.1.20191231-hed1e85f_2\n",
      "  libtiff                                  4.3.0-hd146c10_2 --> 4.2.0-h1167814_3\n",
      "  ncurses                                    6.2-h2e338ed_4 --> 6.1-h0a44026_1002\n",
      "  pillow                               8.4.0-py37h76dc067_0 --> 8.2.0-py37hd4e48bc_1\n",
      "  python                        3.7.12-haf480d7_100_cpython --> 3.7.8-hc9dea61_1_cpython\n",
      "  qt                                      5.12.9-h126340a_4 --> 5.12.9-h126340a_2\n",
      "  readline                                   8.1-h05e3726_0 --> 8.0-hed1e85f_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "dbus-1.13.6          | 558 KB    | ##################################### | 100% \n",
      "libpq-12.9           | 2.0 MB    | ##################################### | 100% \n",
      "qt-5.12.9            | 87.8 MB   | ##################################### | 100% \n",
      "ncurses-6.1          | 1.3 MB    | ##################################### | 100% \n",
      "bzip2-1.0.8          | 155 KB    | ##################################### | 100% \n",
      "libedit-3.1.20191231 | 103 KB    | ##################################### | 100% \n",
      "libffi-3.2.1         | 41 KB     | ##################################### | 100% \n",
      "samtools-1.9         | 304 KB    | ##################################### | 100% \n",
      "gettext-0.19.8.1     | 3.2 MB    | ##################################### | 100% \n",
      "libtiff-4.2.0        | 592 KB    | ##################################### | 100% \n",
      "pillow-8.2.0         | 649 KB    | ##################################### | 100% \n",
      "sqlite-3.37.2        | 1.2 MB    | ##################################### | 100% \n",
      "python-3.7.8         | 23.8 MB   | ##################################### | 100% \n",
      "glib-2.66.1          | 3.2 MB    | ##################################### | 100% \n",
      "libdeflate-1.0       | 45 KB     | ##################################### | 100% \n",
      "readline-8.0         | 255 KB    | ##################################### | 100% \n",
      "htslib-1.9           | 1.2 MB    | ##################################### | 100% \n",
      "cffi-1.14.4          | 218 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c bioconda samtools=1.9 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bamnostic as bs\n",
    "from collections import Counter\n",
    "from scipy.stats import binom_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mapping to a genome\n",
    "\n",
    "In our previous class, we developed our own tool for mapping reads to a genome using the efficient BWT algorithm. Today, we will be using an existing implementation of this algorithm to align many reads to a small genome. \n",
    "\n",
    "For the next few classes, we will be working with a small genome that we will assume represents a sample from a diploid individual from a population. The genome itself is quite small at ~9kb and contains only a few genes to make analysis during class tractable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGAACCCACTGCTTAAGCCTCAATAAAGCTTGCCTTGAGTGCTTCAAGTAGTGTGTGCCCGTCTGTTGTGTGACTCTGGTAACTAGAGATCCCTCAGACCCTTTTAGTCAGTGTGGAAAATCTCTAGCAGTGGCGCCCGAACAGGGACCTGAAAGCGAAAGGGAAACCAGAGGAGCTCTCTCGACGCAGGACTCGGCTTGCTGAAGCGCGCACGGCAAGAGGCGAGGGGCGGCGACTGGTGAGTACGCC\n"
     ]
    }
   ],
   "source": [
    "# We can use our original get_fasta function to examine the fasta file for the genome\n",
    "from data_readers import get_fasta\n",
    "file = \"data/sample_genome.fna\"\n",
    "\n",
    "for name, seq in get_fasta(file):\n",
    "    print(seq[1:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be creating an index using BWT so that we can align reads. To accomplish this, we could use the code from class. However, because we only allow for exact matches, we wouldn't be able to identify variants in our data. Instead, we will be using an aligner that uses the same algorithm that we implemented but allows for some mismatches to the genome called Bowtie2 ( http://bowtie-bio.sourceforge.net/bowtie2/index.shtml ). Because this is building the index of the reference genome (the Burrows-Wheeler transform), you only have to do this once for our genome.\n",
    "\n",
    "First, create an index of the reference genome using `bowtie2-build`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No input sequence or sequence file specified!\n",
      "Bowtie 2 version 2.3.5.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\n",
      "Usage: bowtie2-build [options]* <reference_in> <bt2_index_base>\n",
      "    reference_in            comma-separated list of files with ref sequences\n",
      "    bt2_index_base          write bt2 data to files with this dir/basename\n",
      "*** Bowtie 2 indexes work only with v2 (not v1).  Likewise for v1 indexes. ***\n",
      "Options:\n",
      "    -f                      reference files are Fasta (default)\n",
      "    -c                      reference sequences given on cmd line (as\n",
      "                            <reference_in>)\n",
      "    --large-index           force generated index to be 'large', even if ref\n",
      "                            has fewer than 4 billion nucleotides\n",
      "    --debug                 use the debug binary; slower, assertions enabled\n",
      "    --sanitized             use sanitized binary; slower, uses ASan and/or UBSan\n",
      "    --verbose               log the issued command\n",
      "    -a/--noauto             disable automatic -p/--bmax/--dcv memory-fitting\n",
      "    -p/--packed             use packed strings internally; slower, less memory\n",
      "    --bmax <int>            max bucket sz for blockwise suffix-array builder\n",
      "    --bmaxdivn <int>        max bucket sz as divisor of ref len (default: 4)\n",
      "    --dcv <int>             diff-cover period for blockwise (default: 1024)\n",
      "    --nodc                  disable diff-cover (algorithm becomes quadratic)\n",
      "    -r/--noref              don't build .3/.4 index files\n",
      "    -3/--justref            just build .3/.4 index files\n",
      "    -o/--offrate <int>      SA is sampled every 2^<int> BWT chars (default: 5)\n",
      "    -t/--ftabchars <int>    # of chars consumed in initial lookup (default: 10)\n",
      "    --threads <int>         # of threads\n",
      "    --seed <int>            seed for random number generator\n",
      "    -q/--quiet              verbose output (for debugging)\n",
      "    -h/--help               print detailed description of tool and its options\n",
      "    --usage                 print this usage message\n",
      "    --version               print version information and quit\n"
     ]
    }
   ],
   "source": [
    "# This gives us the usage information for bowtie2-build\n",
    "! bowtie2-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"sample_genome.*.bt2\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 4 (one in 16)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  data/sample_genome.fna\n",
      "Building a SMALL index\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 2295\n",
      "Using parameters --bmax 1722 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 1722 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 9181 (target: 1721)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 1\n",
      "  No samples; assembling all-inclusive block\n",
      "  Sorting block of length 9181 for bucket 1\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 9182 for bucket 1\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 3272\n",
      "fchr[G]: 4914\n",
      "fchr[T]: 7139\n",
      "fchr[$]: 9181\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 4197544 bytes to primary EBWT file: sample_genome.1.bt2\n",
      "Wrote 2300 bytes to secondary EBWT file: sample_genome.2.bt2\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 9181\n",
      "    bwtLen: 9182\n",
      "    sz: 2296\n",
      "    bwtSz: 2296\n",
      "    lineRate: 6\n",
      "    offRate: 4\n",
      "    offMask: 0xfffffff0\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 574\n",
      "    offsSz: 2296\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 48\n",
      "    sideBwtLen: 192\n",
      "    numSides: 48\n",
      "    numLines: 48\n",
      "    ebwtTotLen: 3072\n",
      "    ebwtTotSz: 3072\n",
      "    color: 0\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:00\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "  Time to reverse reference sequence: 00:00:00\n",
      "bmax according to bmaxDivN setting: 2295\n",
      "Using parameters --bmax 1722 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 1722 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 9181 (target: 1721)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 1\n",
      "  No samples; assembling all-inclusive block\n",
      "  Sorting block of length 9181 for bucket 1\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 9182 for bucket 1\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 3272\n",
      "fchr[G]: 4914\n",
      "fchr[T]: 7139\n",
      "fchr[$]: 9181\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 4197544 bytes to primary EBWT file: sample_genome.rev.1.bt2\n",
      "Wrote 2300 bytes to secondary EBWT file: sample_genome.rev.2.bt2\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 9181\n",
      "    bwtLen: 9182\n",
      "    sz: 2296\n",
      "    bwtSz: 2296\n",
      "    lineRate: 6\n",
      "    offRate: 4\n",
      "    offMask: 0xfffffff0\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 574\n",
      "    offsSz: 2296\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 48\n",
      "    sideBwtLen: 192\n",
      "    numSides: 48\n",
      "    numLines: 48\n",
      "    ebwtTotLen: 3072\n",
      "    ebwtTotSz: 3072\n",
      "    color: 0\n",
      "    reverse: 1\n",
      "Total time for backward call to driver() for mirror index: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# From above, the correct format for building an index is:\n",
    "# bowtie2-build <our genome FASTA file> <name of the index we create>\n",
    "! bowtie2-build data/sample_genome.fna sample_genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created an index, you can map our reads to the genome. We have a set of simulated illumina DNA reads from this genome available in `data/sample_reads.fa`. To accomplish this, use `bowtie2` and write your files to `sample_reads.sam` using the `-S` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No index, query, or output file specified!\n",
      "Bowtie 2 version 2.3.5.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\n",
      "Usage: \n",
      "  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n",
      "\n",
      "  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n",
      "             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n",
      "  <m1>       Files with #1 mates, paired with files in <m2>.\n",
      "             Could be gzip'ed (extension: .gz) or bzip2'ed (extension: .bz2).\n",
      "  <m2>       Files with #2 mates, paired with files in <m1>.\n",
      "             Could be gzip'ed (extension: .gz) or bzip2'ed (extension: .bz2).\n",
      "  <r>        Files with unpaired reads.\n",
      "             Could be gzip'ed (extension: .gz) or bzip2'ed (extension: .bz2).\n",
      "  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n",
      "             Could be gzip'ed (extension: .gz) or bzip2'ed (extension: .bz2).\n",
      "  <bam>      Files are unaligned BAM sorted by read name.\n",
      "  <sam>      File for SAM output (default: stdout)\n",
      "\n",
      "  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n",
      "  specified many times.  E.g. '-U file1.fq,file2.fq -U file3.fq'.\n",
      "\n",
      "Options (defaults in parentheses):\n",
      "\n",
      " Input:\n",
      "  -q                 query input files are FASTQ .fq/.fastq (default)\n",
      "  --tab5             query input files are TAB5 .tab5\n",
      "  --tab6             query input files are TAB6 .tab6\n",
      "  --qseq             query input files are in Illumina's qseq format\n",
      "  -f                 query input files are (multi-)FASTA .fa/.mfa\n",
      "  -r                 query input files are raw one-sequence-per-line\n",
      "  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n",
      "                     are substrings (k-mers) extracted from a FASTA file <s>\n",
      "                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n",
      "  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n",
      "  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n",
      "  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n",
      "  -5/--trim5 <int>   trim <int> bases from 5'/left end of reads (0)\n",
      "  -3/--trim3 <int>   trim <int> bases from 3'/right end of reads (0)\n",
      "  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3' or 5' end\n",
      "                     If the read end is not specified then it defaults to 3 (0)\n",
      "  --phred33          qualities are Phred+33 (default)\n",
      "  --phred64          qualities are Phred+64\n",
      "  --int-quals        qualities encoded as space-delimited integers\n",
      "\n",
      " Presets:                 Same as:\n",
      "  For --end-to-end:\n",
      "   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n",
      "   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n",
      "   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n",
      "   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n",
      "\n",
      "  For --local:\n",
      "   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n",
      "   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n",
      "   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n",
      "   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n",
      "\n",
      " Alignment:\n",
      "  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n",
      "  -L <int>           length of seed substrings; must be >3, <32 (22)\n",
      "  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n",
      "  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n",
      "  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n",
      "  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n",
      "  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n",
      "  --nofw             do not align forward (original) version of read (off)\n",
      "  --norc             do not align reverse-complement version of read (off)\n",
      "  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n",
      "                     scan for the optimal seeded alignments\n",
      "  --end-to-end       entire read must align; no clipping (on)\n",
      "   OR\n",
      "  --local            local alignment; ends might be soft clipped (off)\n",
      "\n",
      " Scoring:\n",
      "  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n",
      "  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n",
      "  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n",
      "  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n",
      "  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n",
      "  --score-min <func> min acceptable alignment score w/r/t read length\n",
      "                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n",
      "\n",
      " Reporting:\n",
      "  (default)          look for multiple alignments, report best, with MAPQ\n",
      "   OR\n",
      "  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n",
      "   OR\n",
      "  -a/--all           report all alignments; very slow, MAPQ not meaningful\n",
      "\n",
      " Effort:\n",
      "  -D <int>           give up extending after <int> failed extends in a row (15)\n",
      "  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n",
      "\n",
      " Paired-end:\n",
      "  -I/--minins <int>  minimum fragment length (0)\n",
      "  -X/--maxins <int>  maximum fragment length (500)\n",
      "  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n",
      "  --no-mixed         suppress unpaired alignments for paired reads\n",
      "  --no-discordant    suppress discordant alignments for paired reads\n",
      "  --dovetail         concordant when mates extend past each other\n",
      "  --no-contain       not concordant when one mate alignment contains other\n",
      "  --no-overlap       not concordant when mates overlap at all\n",
      "\n",
      " BAM:\n",
      "  --align-paired-reads Bowtie2 will, by default, attempt to align unpaired BAM reads.\n",
      "                     Use this option to align paired-end reads instead.\n",
      "  --preserve-tags    Preserve tags from the original BAM record by\n",
      "                     appending them to the end of the corresponding SAM output.\n",
      "\n",
      " Output:\n",
      "  -t/--time          print wall-clock time taken by search phases\n",
      "  --un <path>        write unpaired reads that didn't align to <path>\n",
      "  --al <path>        write unpaired reads that aligned at least once to <path>\n",
      "  --un-conc <path>   write pairs that didn't align concordantly to <path>\n",
      "  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n",
      "    (Note: for --un, --al, --un-conc, or --al-conc, add '-gz' to the option name, e.g.\n",
      "    --un-gz <path>, to gzip compress output, or add '-bz2' to bzip2 compress output.)\n",
      "  --quiet            print nothing to stderr except serious errors\n",
      "  --met-file <path>  send metrics to file at <path> (off)\n",
      "  --met-stderr       send metrics to stderr (off)\n",
      "  --met <int>        report internal counters & metrics every <int> secs (1)\n",
      "  --no-unal          suppress SAM records for unaligned reads\n",
      "  --no-head          suppress header lines, i.e. lines starting with @\n",
      "  --no-sq            suppress @SQ header lines\n",
      "  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n",
      "  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n",
      "                     Note: @RG line only printed when --rg-id is set.\n",
      "  --omit-sec-seq     put '*' in SEQ and QUAL fields for secondary alignments.\n",
      "  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n",
      "                      at the expense of generating non-standard SAM.\n",
      "  --xeq              Use '='/'X', instead of 'M,' to specify matches/mismatches in SAM record.\n",
      "  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n",
      "\n",
      " Performance:\n",
      "  -p/--threads <int> number of alignment threads to launch (1)\n",
      "  --reorder          force SAM output order to match order of input reads\n",
      "  --mm               use memory-mapped I/O for index; many 'bowtie's can share\n",
      "\n",
      " Other:\n",
      "  --qc-filter        filter out reads that are bad according to QSEQ filter\n",
      "  --seed <int>       seed for random number generator (0)\n",
      "  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n",
      "  --version          print version information and quit\n",
      "  -h/--help          print this usage message\n",
      "(ERR): bowtie2-align exited with value 1\n"
     ]
    }
   ],
   "source": [
    "# This gives us the usage information for bowtie2\n",
    "! bowtie2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22952 reads; of these:\n",
      "  22952 (100.00%) were unpaired; of these:\n",
      "    0 (0.00%) aligned 0 times\n",
      "    22519 (98.11%) aligned exactly 1 time\n",
      "    433 (1.89%) aligned >1 times\n",
      "100.00% overall alignment rate\n"
     ]
    }
   ],
   "source": [
    "# From above, the correct format for building an index is:\n",
    "# bowtie2 -f -x <name of the index we created> -U <FASTA of sequence reads> -S <name of SAM output file>\n",
    "# *We use -f because our input is a fasta file\n",
    "! bowtie2 -f -x sample_genome -U data/sample_reads.fa -S sample_reads.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to convert the sam file into a bam file using samtools sort. You can read more about samtools: http://www.htslib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: samtools sort [options...] [in.bam]\n",
      "Options:\n",
      "  -l INT     Set compression level, from 0 (uncompressed) to 9 (best)\n",
      "  -m INT     Set maximum memory per thread; suffix K/M/G recognized [768M]\n",
      "  -n         Sort by read name\n",
      "  -t TAG     Sort by value of TAG. Uses position as secondary index (or read name if -n is set)\n",
      "  -o FILE    Write final output to FILE rather than standard output\n",
      "  -T PREFIX  Write temporary files to PREFIX.nnnn.bam\n",
      "      --input-fmt-option OPT[=VAL]\n",
      "               Specify a single input file format option in the form\n",
      "               of OPTION or OPTION=VALUE\n",
      "  -O, --output-fmt FORMAT[,OPT[=VAL]]...\n",
      "               Specify output format (SAM, BAM, CRAM)\n",
      "      --output-fmt-option OPT[=VAL]\n",
      "               Specify a single output file format option in the form\n",
      "               of OPTION or OPTION=VALUE\n",
      "      --reference FILE\n",
      "               Reference sequence FASTA FILE [null]\n",
      "  -@, --threads INT\n",
      "               Number of additional threads to use [0]\n"
     ]
    }
   ],
   "source": [
    "# This gives us the usage information for samtools sort\n",
    "! samtools sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above, the correct format for building the bam file is:\n",
    "# samtools sort -o <name of bam file output> <name of sam file input>\n",
    "! samtools sort -o sample_reads.bam sample_reads.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now mapped your reads to a reference genome using the Burrows-Wheeler algorithm! You can take a look at the sample_reads.sam file to see the plain text version of the alignments and the sample_reads.bam now contains a compressed version of the same alignments.\n",
    "\n",
    "---\n",
    "## Identifying variants in the genome\n",
    "\n",
    "For the second section of today's class, we will be identifying variants in our diploid genome. We will be using the `bamnostic` package ( https://github.com/betteridiot/bamnostic ) to work with our aligned reads from a bam file.\n",
    "\n",
    "To identify variants, we will test each position for non-reference alleles and perform a binomial test to determine if there is indeed a variant or just a sequencing error at that position. This algorithm is somtimes referred to as a 'pileup'.\n",
    "\n",
    "```\n",
    "find_variants(bam_file):\n",
    "    For each position in genome:\n",
    "        count allele frequencies (pileup)\n",
    "        test for heterozygosity\n",
    "        test for homozygous alternative allele\n",
    "```\n",
    "\n",
    "Note: There are multuple ways to implement this, however we recommend using `Counter()` from `collections` that has been discussed and demonstrated multiple times on the office hours live streams. The binomial test can be implemented directly from the equation below, or you can use scipy.stats.\n",
    "\n",
    "Binomial test is calculated as: $P(X=k) = {n \\choose k}p^{k}(1-p)^{n-k}$ where $k$ is the allele count, $n$ is the total number of reads, and $p$ is 0.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bamnostic as bs\n",
    "from collections import Counter\n",
    "from scipy.stats import binom_test\n",
    "\n",
    "def get_pileup(alignments, region_start = None, region_end = None):\n",
    "    ''' Function build a read pileup list\n",
    "        this is implemented as a list of Counter() from region_start to region_end\n",
    "        with our small genome, it is reasonable to cover the entire genome\n",
    "        but for larger genomes a smaller window is required.\n",
    "    \n",
    "    Args:\n",
    "        alignments (str): bam file of alignments\n",
    "        region_start (int): start position to build pileup\n",
    "        region_end (int): end position to build pileup\n",
    "    \n",
    "    Returns:\n",
    "        genome (list of Counter()): a list from region_start to region_end of\n",
    "            Counters of allele frequencies\n",
    "        \n",
    "    Example:\n",
    "        >>> get_pileup('sample_reads.bam') #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE \n",
    "        [Counter({'G': 5}), Counter({'G': 8}), Counter({'T': 12}), ...] \n",
    "    '''\n",
    "    # firt need to open the bam file. \n",
    "    bam = bs.AlignmentFile(alignments, \"rb\") \n",
    "    \n",
    "    #setting the default to measuring the entire genome, from zero to the very end. \n",
    "    \n",
    "    if region_start == None:\n",
    "        region_start = 0\n",
    "        \n",
    "    if region_end == None:\n",
    "        region_end = bam.header[\"SQ\"][0][\"LN\"] #from the documentation and it's in the header. \n",
    "        #same concept as when we took from a header line in 575. \n",
    "    \n",
    "    #list of the Counter for each base within the given region \n",
    "    genome = [Counter() for _ in range(region_start, region_end)]\n",
    "    \n",
    "    \n",
    "    for read in bam:\n",
    "        #we have an option in enumerate to say when we want to start counting from \n",
    "        #usually start 0, but we can start at a certain position, want to start at the beginning of the alignmnet of the read. \n",
    "        #increment as we go to each position. \n",
    "        for i, base in enumerate(read.seq, read.pos):\n",
    "            if i >= region_start and i <= region_end:\n",
    "                #increment the Counter at that base. So i is really your index. \n",
    "                genome[i-region_start].update(base) \n",
    "                #sets us back to zero being region_start. \n",
    "    \n",
    "    \n",
    "    # print(genome[:10])\n",
    "    # print(genome[-10:])\n",
    "    \n",
    "    # return genome \n",
    "    \n",
    "\n",
    "def binomial_test(major, minor):\n",
    "    ''' Function to perform binomial test\n",
    "        We will consider a Pvalue threshold of 0.10: \n",
    "        SNPs for which the P value of the binomial test < 0.10 failed the heterozygosity test.\n",
    "\n",
    "    Args:\n",
    "        major (int): count of most frequent allele\n",
    "        minor (int): count of second most frequent allele\n",
    "    \n",
    "    Returns:\n",
    "        is_above_threshold (bool): true if passes heterozygosity test, otherwise false\n",
    "        \n",
    "    Example:\n",
    "        >>> binomial_test(8, 4)\n",
    "        True\n",
    "    '''\n",
    "    #will perform binomial test \n",
    "    #Binomial test is calculated as:  𝑃(𝑋=𝑘)=(𝑛𝑘)𝑝𝑘(1−𝑝)𝑛−𝑘  where  𝑘  is the allele count,  𝑛  is the total number of reads, and  𝑝  is 0.50\n",
    "    \n",
    "    \n",
    "    p_val = binom_test(major, major + minor, 1/2) \n",
    "    if p_val > 0.10:\n",
    "        return True \n",
    "    \n",
    "    #else return False. \n",
    "    return False \n",
    "    \n",
    "\n",
    "def find_variants(reference, alignments):\n",
    "    ''' Function to find variants given sequencing alignments and a reference\n",
    "        Identify variants that are heterozygous using heterozygosity test\n",
    "        Identify variants that are homozygous alternative allele\n",
    "        \n",
    "        Note: Variants are reported as 1-based coordinates\n",
    "        \n",
    "    Args:\n",
    "        reference (str): genome reference fasta file\n",
    "        alignments (str): sequence alignments\n",
    "    \n",
    "    Returns:\n",
    "        variant_list (list of tuples): list of variants as (position, allele1, allele2)\n",
    "        \n",
    "    Example:\n",
    "        >>> find_variants(reference = 'data/sample_genome.fna', alignments = 'sample_reads.bam') #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE \n",
    "        [(240, 'A', 'G'), (354, 'G', 'A'), (803, 'C', 'A'), ...]\n",
    "    '''\n",
    "    \n",
    "    \n",
    "#     find_variants(bam_file):\n",
    "#     For each position in genome:\n",
    "#         count allele frequencies (pileup)\n",
    "#         test for heterozygosity\n",
    "#         test for homozygous alternative allele\n",
    "    \n",
    "    #need an empty list for our variants. \n",
    "    variant_list = []\n",
    "    \n",
    "    #get our genome, which we can from the reference. \n",
    "    #need genome seq to track variants. get that from the reference which is a fasta file. \n",
    "    \n",
    "    \n",
    "    #Reference genome right here!!\n",
    "    genome = None \n",
    "    for name, seq in get_fasta(reference):\n",
    "        genome = seq \n",
    "    #loops through it once to get the genome sequence. \n",
    "    \n",
    "    #now need to calculate the pileup. Genome_pileup. \n",
    "    genome_pileup = get_pileup(alignments, region_start = None, region_end = len(genome))\n",
    "    \n",
    "    #iterate through the gneome to identify variants. \n",
    "    for j, counter in enumerate(genome_pileup, 1): #start at 1 since variants are reported as 1-based. \n",
    "        #need the reference allele \n",
    "        ref_allele = genome[j-1] #i-1 position since we're now indexing by one off. genome comes from fasta file. \n",
    "        top_alleles = counter.most_common(2) #get the top two alleles. \n",
    "        \n",
    "        #test for heterozygosity need more than one at the same position. \n",
    "        if len(counter) > 1:\n",
    "            if binomial_test(top_alleles[0][1], top_alleles[1][1]): #first item in the counter and second item in the counter. \n",
    "                variant_list.append((j, top_alleles[0][0], top_alleles[1][0])) #this is what they want to get out. #we want the bases themselves, not the counts.\n",
    "            elif len(counter) == 0:  #there may not be any reads at a point, so we're just going to pass. \n",
    "                next \n",
    "            else: #test for homozygous, alternate site. \n",
    "                if counter[ref_allele] == 0:\n",
    "                    variant_list.append((j, top_alleles[0][0], top_alleles[0][0]))\n",
    "                \n",
    "    return variant_list \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_02 = get_pileup('sample_reads.bam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(240, 'A', 'G'), (354, 'G', 'A'), (803, 'C', 'A'), (1411, 'A', 'G'), (1799, 'C', 'G'), (1829, 'G', 'C'), (1910, 'T', 'C'), (1958, 'A', 'T'), (2119, 'A', 'G'), (2327, 'G', 'A'), (2404, 'A', 'G'), (2425, 'T', 'A'), (2605, 'C', 'T'), (2678, 'G', 'A'), (2788, 'A', 'T'), (2965, 'A', 'C'), (3017, 'T', 'G'), (3141, 'G', 'T'), (3383, 'G', 'C'), (3536, 'G', 'A'), (3779, 'G', 'T'), (3822, 'C', 'A'), (3908, 'G', 'C'), (4497, 'C', 'T'), (4548, 'A', 'C'), (4737, 'G', 'A'), (4823, 'G', 'A'), (5144, 'C', 'G'), (5275, 'G', 'T'), (5313, 'T', 'C'), (5522, 'A', 'G'), (5586, 'G', 'A'), (5624, 'T', 'A'), (5652, 'A', 'C'), (5910, 'C', 'A'), (6013, 'A', 'T'), (6303, 'A', 'G'), (6632, 'G', 'C'), (6734, 'A', 'C'), (7099, 'C', 'T'), (7376, 'A', 'C'), (7466, 'C', 'T'), (7970, 'C', 'G'), (8116, 'G', 'T'), (8294, 'T', 'C'), (8545, 'T', 'C'), (8788, 'C', 'A'), (8981, 'G', 'C')]\n"
     ]
    }
   ],
   "source": [
    "print(find_variants(reference = 'data/sample_genome.fna', alignments = 'sample_reads.bam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 26, 'G': 24})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = get_pileup(\"sample_reads.bam\")\n",
    "test1[239]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
